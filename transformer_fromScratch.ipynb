{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T17:38:55.376144Z",
     "start_time": "2025-03-16T17:38:48.240085Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from html import unescape\n",
    "import re\n",
    "import spacy\n",
    "from titlecase import titlecase\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:38:55.835727Z",
     "start_time": "2025-03-16T17:38:55.383744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data\n",
    "processed_path = \"./processed_data.parquet\"\n",
    "df = pd.read_parquet(processed_path)\n",
    "\n",
    "# load vocab\n",
    "vocab_df = pd.read_csv(\"./vocab.csv\")\n",
    "vocab = {word: idx for word, idx in zip(vocab_df[\"word\"], vocab_df[\"id\"])}\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}  # get reverse vocab\n",
    "\n",
    "pad_idx = vocab['<pad>']\n",
    "unk_idx = vocab['<unk>']\n",
    "sos_idx = vocab['<start>']\n",
    "eos_idx = vocab['<end>']"
   ],
   "id": "7f814f827885a9a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:38:56.178039Z",
     "start_time": "2025-03-16T17:38:56.172031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List of (input_seq, target_seq)\n",
    "    \"\"\"\n",
    "    input_seqs  = [torch.tensor(item[0], dtype=torch.long) for item in batch]\n",
    "    target_seqs = [torch.tensor(item[1], dtype=torch.long) for item in batch]\n",
    "\n",
    "    # padding to the max length in batch\n",
    "    padded_inputs = pad_sequence(input_seqs,\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=pad_idx)\n",
    "\n",
    "    padded_targets = pad_sequence(target_seqs,\n",
    "                                  batch_first=True,\n",
    "                                  padding_value=pad_idx)\n",
    "\n",
    "    return padded_inputs, padded_targets"
   ],
   "id": "da3206970d0a4fbc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:38:56.205109Z",
     "start_time": "2025-03-16T17:38:56.196346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, dataframe, input_col=\"desc_seq\", target_col=\"title_seq\",\n",
    "                 max_input_len=200, max_target_len=20):\n",
    "        self.sos_idx = vocab['<start>']\n",
    "        self.eos_idx = vocab['<end>']\n",
    "        self.pad_idx = vocab['<pad>']\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "        self.samples = []\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            input_seq = self._process_seq(row[input_col].tolist(), self.max_input_len, is_target=False)\n",
    "            target_seq = self._process_seq(row[target_col].tolist(), self.max_target_len, is_target=True)\n",
    "\n",
    "            if self._is_valid(input_seq) and self._is_valid(target_seq):\n",
    "                self.samples.append((input_seq, target_seq))\n",
    "\n",
    "    def _process_seq(self, raw_seq, max_len, is_target):\n",
    "        \"\"\"ensure the seq structure：[<start>, tokens..., <end>, <pad>...]\"\"\"\n",
    "        filtered = [x for x in raw_seq if x not in (self.sos_idx, self.eos_idx, self.pad_idx)]\n",
    "\n",
    "        content_max_len = max_len - 2  \n",
    "        truncated = filtered[:content_max_len]\n",
    "\n",
    "        processed = [self.sos_idx] + truncated + [self.eos_idx]\n",
    "\n",
    "        # padding to max_len\n",
    "        if len(processed) < max_len:\n",
    "            processed += [self.pad_idx] * (max_len - len(processed))\n",
    "        else:\n",
    "            processed = processed[:max_len]  \n",
    "            processed[-1] = self.eos_idx  \n",
    "\n",
    "        return processed\n",
    "\n",
    "    def _is_valid(self, seq):\n",
    "        \"\"\"ensure the seq structure：[<start>, tokens..., <end>]\"\"\"\n",
    "        has_start = self.sos_idx in seq\n",
    "        has_end = self.eos_idx in seq\n",
    "        start_pos = seq.index(self.sos_idx) if has_start else -1\n",
    "        end_pos = seq.index(self.eos_idx) if has_end else -1\n",
    "        content = seq[start_pos+1:end_pos] if (has_start and has_end) else []\n",
    "        return has_start and has_end and (len(content) >= 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ],
   "id": "ac7ff6d422d771d2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.542219Z",
     "start_time": "2025-03-16T17:38:56.273780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = AGNewsDataset(train_df, input_col=\"desc_seq\", target_col=\"title_seq\",\n",
    "                              max_input_len=200, max_target_len=20)\n",
    "test_dataset  = AGNewsDataset(test_df, input_col=\"desc_seq\", target_col=\"title_seq\",\n",
    "                              max_input_len=200, max_target_len=20)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=128, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "b18a089bd8e61f0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.578007Z",
     "start_time": "2025-03-16T17:39:02.568178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 37943  \n",
    "        self.pad_idx = 0\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "\n",
    "        self.d_model = 512\n",
    "        self.nhead = 8\n",
    "        self.num_encoder_layers = 6\n",
    "        self.num_decoder_layers = 6\n",
    "        self.dim_feedforward = 2048\n",
    "        self.dropout = 0.2\n",
    "\n",
    "        self.enc_max_len = 200\n",
    "        self.dec_max_len = 20\n",
    "\n",
    "        self.share_final_proj = False"
   ],
   "id": "6569a55044a21003",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.608563Z",
     "start_time": "2025-03-16T17:39:02.603845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generate boolean causal mask for transformer decoder\"\"\"\n",
    "    mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
    "    return mask  # [sz, sz], dtype=torch.bool"
   ],
   "id": "fbe228c5d286b954",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.640230Z",
     "start_time": "2025-03-16T17:39:02.633419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.2, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initialize positional encoding matrix [max_len, d_model]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "            (-math.log(10000.0) / d_model)  \n",
    "        )  # [d_model/2]\n",
    "\n",
    "        # Apply trigonometric functions to even and odd positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sine\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cosine\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        # Numerical range validation (commented out by default)\n",
    "        # print(f\"PE value range - min: {pe.min().item():.4f}, max: {pe.max().item():.4f}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n",
    "        return self.dropout(x)"
   ],
   "id": "3be07af14796ca4d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.672415Z",
     "start_time": "2025-03-16T17:39:02.664213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = d_model // nhead\n",
    "\n",
    "        # Define linear projection layers\n",
    "        self.Wq = nn.Linear(d_model, d_model)  # Query projection\n",
    "        self.Wk = nn.Linear(d_model, d_model)  # Key projection\n",
    "        self.Wv = nn.Linear(d_model, d_model)  # Value projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)  # Final output projection\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                key_padding_mask: torch.Tensor = None,  # Shape: [batch_size, seq_len]\n",
    "                attn_mask: torch.Tensor = None):         # Shape: [tgt_len, src_len]\n",
    "        \"\"\"\n",
    "        query: [batch_size, tgt_len, d_model]\n",
    "        key:   [batch_size, src_len, d_model]\n",
    "        value: [batch_size, src_len, d_model]\n",
    "        \n",
    "        returns：\n",
    "        attn_output: [batch_size, tgt_len, d_model]\n",
    "        attn_weights: [batch_size, nhead, tgt_len, src_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Step 1: Linear projections and multi-head splitting\n",
    "        Q = self.Wq(query).view(batch_size, -1, self.nhead, self.head_dim).transpose(1, 2)  # [B, nh, T, hd]\n",
    "        K = self.Wk(key).view(batch_size, -1, self.nhead, self.head_dim).transpose(1, 2)      # [B, nh, S, hd]\n",
    "        V = self.Wv(value).view(batch_size, -1, self.nhead, self.head_dim).transpose(1, 2)   # [B, nh, S, hd]\n",
    "\n",
    "        # Step 2: Compute scaled dot-product attention\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, nh, T, S]\n",
    "\n",
    "        # Step 3: Apply attention mask (causal mask)\n",
    "        if attn_mask is not None:\n",
    "            # [T, S] -> [B, nh, T, S]\n",
    "            attn_scores.masked_fill_(attn_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        # Step 4: Apply key padding mask\n",
    "        if key_padding_mask is not None:\n",
    "            # [B, S] -> [B, 1, 1, S]\n",
    "            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, -1)\n",
    "            attn_scores = attn_scores.masked_fill(key_padding_mask, float('-inf'))\n",
    "\n",
    "        # Step 5: Compute attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # [B, nh, T, S]\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Step 6: Compute context vectors\n",
    "        context = torch.matmul(attn_weights, V)  # [B, nh, T, hd]\n",
    "\n",
    "        # Step 7: Combine multi-head outputs\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # [B, T, d_model]\n",
    "\n",
    "        # Step 8: Final output projection\n",
    "        attn_output = self.out_proj(context)\n",
    "\n",
    "        return attn_output, attn_weights"
   ],
   "id": "60b672466f3dabc8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.702569Z",
     "start_time": "2025-03-16T17:39:02.696115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, dim_feedforward: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # Initialize parameters (optional but recommended)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Xavier uniform initialization for linear layers\"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.constant_(self.linear1.bias, 0.)\n",
    "        nn.init.constant_(self.linear2.bias, 0.)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)        # [B, L, dim_feedforward]\n",
    "        x = self.activation(x)      # Non-linear activation\n",
    "        x = self.dropout(x)         # Apply dropout regularization\n",
    "        x = self.linear2(x)         # [B, L, d_model]\n",
    "        return x"
   ],
   "id": "b1b362fac0557342",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.733117Z",
     "start_time": "2025-03-16T17:39:02.727662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.ff = FeedForward(\n",
    "            d_model=config.d_model,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(config.d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor,\n",
    "                src_mask: torch.Tensor = None,\n",
    "                src_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Input sequence [batch_size, src_seq_len, d_model]\n",
    "            src_mask: Optional attention mask [src_seq_len, src_seq_len] \n",
    "                     (typically not needed for encoder)\n",
    "            src_key_padding_mask: Boolean mask [batch_size, src_seq_len] \n",
    "                                 (True indicates padding positions)\n",
    "        \n",
    "        Returns:\n",
    "            Encoded output [batch_size, src_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_output, _ = self.self_attn(\n",
    "            query=src,  # [B, L, D]\n",
    "            key=src,    # [B, L, D]\n",
    "            value=src,  # [B, L, D]\n",
    "            attn_mask=src_mask,              \n",
    "            key_padding_mask=src_key_padding_mask  # Filter padding positions\n",
    "        )\n",
    "\n",
    "        # Residual connection + LayerNorm\n",
    "        src = src + self.dropout(attn_output)  # [B, L, D] + [B, L, D] → [B, L, D]\n",
    "        src = self.norm1(src)  # Maintain dimension\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        ff_output = self.ff(src)  # [B, L, D] → [B, L, D]\n",
    "\n",
    "        # Residual connection + LayerNorm\n",
    "        src = src + self.dropout(ff_output)  # [B, L, D] + [B, L, D] → [B, L, D]\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        return src"
   ],
   "id": "3ca8525e8bfbc714",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.766142Z",
     "start_time": "2025-03-16T17:39:02.760243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.d_model,\n",
    "            padding_idx=config.pad_idx\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=config.d_model,\n",
    "            dropout=config.dropout,\n",
    "            max_len=config.enc_max_len\n",
    "        )\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(config)\n",
    "            for _ in range(config.num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src: torch.Tensor,\n",
    "                src_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Input token indices [batch_size, src_seq_len]\n",
    "            src_key_padding_mask: Boolean mask [batch_size, src_seq_len] \n",
    "                                 (True indicates padding positions)\n",
    "        \n",
    "        Returns:\n",
    "            Encoded sequence [batch_size, src_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # 1. Embedding layer\n",
    "        src = self.embedding(src)  # [B, L] → [B, L, D]\n",
    "        src = self.pos_encoder(src)  # Add positional encodings\n",
    "\n",
    "        # 2. Process through encoder layers\n",
    "        for layer in self.layers:\n",
    "            src = layer(\n",
    "                src=src,\n",
    "                src_mask=None,  # Encoder doesn't need causal masking\n",
    "                src_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "\n",
    "        return src"
   ],
   "id": "569a955c5b72a5a6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.800406Z",
     "start_time": "2025-03-16T17:39:02.792740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Self-attention mechanism (processes target sequence)\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        # Encoder-decoder cross attention mechanism\n",
    "        self.cross_attn = MultiHeadAttention(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        # Feed-forward network\n",
    "        self.ff = FeedForward(\n",
    "            d_model=config.d_model,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        # Three normalization layers\n",
    "        self.norm1 = nn.LayerNorm(config.d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.d_model)\n",
    "        self.norm3 = nn.LayerNorm(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor = None,\n",
    "                memory_mask: torch.Tensor = None,\n",
    "                tgt_key_padding_mask: torch.Tensor = None,\n",
    "                memory_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Target sequence [batch_size, tgt_seq_len, d_model]\n",
    "            memory: Encoder output [batch_size, src_seq_len, d_model]\n",
    "            tgt_mask: Target sequence causal mask [tgt_seq_len, tgt_seq_len]\n",
    "            tgt_key_padding_mask: Target padding mask [batch_size, tgt_seq_len]\n",
    "            memory_key_padding_mask: Source padding mask [batch_size, src_seq_len]\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_output, _ = self.self_attn(\n",
    "            query=tgt,\n",
    "            key=tgt,\n",
    "            value=tgt,\n",
    "            attn_mask=tgt_mask,                 # Causal mask\n",
    "            key_padding_mask=tgt_key_padding_mask  # Target padding\n",
    "        )\n",
    "        tgt = tgt + self.dropout(attn_output)\n",
    "        tgt = self.norm1(tgt)  # [B, T, D]\n",
    "\n",
    "        # Encoder-decoder attention sublayer\n",
    "        cross_output, _ = self.cross_attn(\n",
    "            query=tgt,          # Query from decoder\n",
    "            key=memory,         # Key from encoder\n",
    "            value=memory,       # Value from encoder\n",
    "            key_padding_mask=memory_key_padding_mask  # Source padding\n",
    "        )\n",
    "        tgt = tgt + self.dropout(cross_output)\n",
    "        tgt = self.norm2(tgt)  # [B, T, D]\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        ff_output = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout(ff_output)\n",
    "        tgt = self.norm3(tgt)  # [B, T, D]\n",
    "\n",
    "        return tgt"
   ],
   "id": "b2a18777b0978cba",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.834881Z",
     "start_time": "2025-03-16T17:39:02.828302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.d_model,\n",
    "            padding_idx=config.pad_idx\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=config.d_model,\n",
    "            dropout=config.dropout,\n",
    "            max_len=config.dec_max_len\n",
    "        )\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(config)\n",
    "            for _ in range(config.num_decoder_layers)\n",
    "        ])\n",
    "        # Final output projection to vocabulary size\n",
    "        self.output_proj = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor = None,\n",
    "                memory_key_padding_mask: torch.Tensor = None,\n",
    "                tgt_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Target sequence token indices [batch_size, tgt_seq_len]\n",
    "            memory: Encoder output [batch_size, src_seq_len, d_model]\n",
    "            tgt_mask: Target attention mask\n",
    "            memory_key_padding_mask: Memory padding mask\n",
    "            tgt_key_padding_mask: Target padding mask\n",
    "            \n",
    "        Returns:\n",
    "            Output logits [batch_size, tgt_seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Embedding layer with positional encoding\n",
    "        tgt = self.embedding(tgt)  # [B, T] → [B, T, D]\n",
    "        tgt = self.pos_encoder(tgt)  # [B, T, D]\n",
    "\n",
    "        # 逐层处理\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(\n",
    "                tgt=tgt,\n",
    "                memory=memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask\n",
    "            )\n",
    "\n",
    "        # Final projection to vocabulary dimension\n",
    "        output = self.output_proj(tgt)  # [B, T, vocab_size]\n",
    "        return output"
   ],
   "id": "4f82fec6f382ce8f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:02.870429Z",
     "start_time": "2025-03-16T17:39:02.861568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Encoder components\n",
    "        self.encoder = Encoder(config)\n",
    "\n",
    "        # Decoder components\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "        # Weight sharing logic (optional)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Weight initialization strategy\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        # Share embedding matrix between encoder/decoder\n",
    "        self.decoder.embedding.weight = self.encoder.embedding.weight\n",
    "        if self.config.share_final_proj:\n",
    "            self.decoder.output_proj.weight = self.encoder.embedding.weight\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_key_padding_mask: torch.Tensor = None,\n",
    "                tgt_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Full forward propagation process\n",
    "        Args:\n",
    "            src: Source sequence [batch_size, src_len]\n",
    "            tgt: Target sequence [batch_size, tgt_len]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, tgt_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Generate causal mask for target sequence\n",
    "        tgt_mask = self.generate_tgt_mask(tgt.size(1)).to(src.device)  # [tgt_len, tgt_len]\n",
    "\n",
    "        # Encoding phase\n",
    "        memory = self.encoder(\n",
    "            src=src,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [B, S, D]\n",
    "\n",
    "        # Decoding phase\n",
    "        logits = self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )  # [B, T, V]\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_tgt_mask(size: int) -> torch.Tensor:\n",
    "        \"\"\"Generate causal mask (upper triangular as True)\"\"\"\n",
    "        return generate_square_subsequent_mask(size)\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_key_padding_mask: torch.Tensor) -> torch.Tensor:  \n",
    "        \"\"\"Standalone encoding method (for inference)\"\"\"\n",
    "        return self.encoder(src, src_key_padding_mask=src_key_padding_mask) \n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor,\n",
    "               tgt_mask: torch.Tensor, memory_key_padding_mask: torch.Tensor) -> torch.Tensor:  \n",
    "        \"\"\"Standalone decoding method (for inference)\"\"\"\n",
    "        return self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask  \n",
    "        )"
   ],
   "id": "4d400d3e4e6789a7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:05.712908Z",
     "start_time": "2025-03-16T17:39:02.895815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = TransformerConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(config).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "d_model = 512\n",
    "warmup_steps = 7000 # 10 epochs\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=d_model**-0.5,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: min(\n",
    "        (step + 1)**-0.5,\n",
    "        (step + 1) * (warmup_steps**-1.5)\n",
    "    )\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=config.pad_idx)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, scheduler, device):  # 添加scheduler参数\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        decoder_input = tgt[:, :-1]  # remove the last token\n",
    "        labels = tgt[:, 1:]          # remove the first token\n",
    "\n",
    "        # Create masks\n",
    "        src_pad_mask = (src == config.pad_idx)\n",
    "        tgt_pad_mask = (decoder_input == config.pad_idx)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(\n",
    "            src=src,\n",
    "            tgt=decoder_input,  \n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask\n",
    "        )\n",
    "        loss = criterion(\n",
    "            logits.contiguous().view(-1, config.vocab_size),\n",
    "            labels.contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate per iteration\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # Display current learning rate\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            src_mask = (src == config.pad_idx)\n",
    "            tgt_mask = (tgt == config.pad_idx)\n",
    "\n",
    "            logits = model(src, tgt[:, :-1],\n",
    "                           src_key_padding_mask=src_mask,\n",
    "                           tgt_key_padding_mask=tgt_mask[:, :-1])\n",
    "\n",
    "            loss = criterion(logits.view(-1, config.vocab_size),\n",
    "                             tgt[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "abeba4cc1999f383",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:05.766705Z",
     "start_time": "2025-03-16T17:39:05.753437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ],
   "id": "d66d8dafb0d184e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83030071"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:39:13.584475Z",
     "start_time": "2025-03-16T17:39:05.846166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "epochs = 100  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # Training phase (pass scheduler)\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss = evaluate(model, test_dataloader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Save best model (when epoch >80)\n",
    "    if epoch > 80:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_loss,\n",
    "            }, f\"transformer-generation/best_model_epoch{epoch+1}_loss{val_loss:.4f}.pth\")\n",
    "            print(f\"Saving new best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.2e}\")"
   ],
   "id": "bc879a0ec90dd9ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.5397 | Val Loss: 7.3489 | LR: 5.31e-05\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.2699 | Val Loss: 7.2803 | LR: 1.06e-04\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.1605 | Val Loss: 7.2464 | LR: 1.59e-04\n",
      "\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.1079 | Val Loss: 7.2589 | LR: 2.12e-04\n",
      "\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.0697 | Val Loss: 7.1851 | LR: 2.65e-04\n",
      "\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.0238 | Val Loss: 7.1570 | LR: 3.18e-04\n",
      "\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.0008 | Val Loss: 7.1240 | LR: 3.71e-04\n",
      "\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9876 | Val Loss: 7.1162 | LR: 4.24e-04\n",
      "\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9792 | Val Loss: 7.1462 | LR: 4.78e-04\n",
      "\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9752 | Val Loss: 7.1576 | LR: 5.27e-04\n",
      "\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9618 | Val Loss: 7.2107 | LR: 5.03e-04\n",
      "\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9888 | Val Loss: 7.2011 | LR: 4.81e-04\n",
      "\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9687 | Val Loss: 7.1591 | LR: 4.62e-04\n",
      "\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9397 | Val Loss: 7.1425 | LR: 4.45e-04\n",
      "\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9074 | Val Loss: 7.1142 | LR: 4.30e-04\n",
      "\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9173 | Val Loss: 7.0679 | LR: 4.17e-04\n",
      "\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9392 | Val Loss: 7.0773 | LR: 4.04e-04\n",
      "\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8936 | Val Loss: 7.0860 | LR: 3.93e-04\n",
      "\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8659 | Val Loss: 7.0933 | LR: 3.82e-04\n",
      "\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8618 | Val Loss: 7.0956 | LR: 3.73e-04\n",
      "\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8434 | Val Loss: 7.0684 | LR: 3.64e-04\n",
      "\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8160 | Val Loss: 7.0708 | LR: 3.55e-04\n",
      "\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8077 | Val Loss: 7.0575 | LR: 3.48e-04\n",
      "\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7836 | Val Loss: 7.0470 | LR: 3.40e-04\n",
      "\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7639 | Val Loss: 7.0202 | LR: 3.33e-04\n",
      "\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7547 | Val Loss: 7.0083 | LR: 3.27e-04\n",
      "\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7308 | Val Loss: 6.9910 | LR: 3.21e-04\n",
      "\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7241 | Val Loss: 6.9820 | LR: 3.15e-04\n",
      "\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7131 | Val Loss: 6.9860 | LR: 3.10e-04\n",
      "\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7121 | Val Loss: 7.0002 | LR: 3.04e-04\n",
      "\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6963 | Val Loss: 6.9843 | LR: 2.99e-04\n",
      "\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6749 | Val Loss: 6.9905 | LR: 2.95e-04\n",
      "\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6648 | Val Loss: 6.9885 | LR: 2.90e-04\n",
      "\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6488 | Val Loss: 7.0083 | LR: 2.86e-04\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6367 | Val Loss: 6.9750 | LR: 2.82e-04\n",
      "\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6332 | Val Loss: 6.9655 | LR: 2.78e-04\n",
      "\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6356 | Val Loss: 6.9685 | LR: 2.74e-04\n",
      "\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6134 | Val Loss: 6.9706 | LR: 2.70e-04\n",
      "\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6094 | Val Loss: 6.9656 | LR: 2.67e-04\n",
      "\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6025 | Val Loss: 6.9533 | LR: 2.64e-04\n",
      "\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6012 | Val Loss: 6.9612 | LR: 2.60e-04\n",
      "\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5832 | Val Loss: 6.9695 | LR: 2.57e-04\n",
      "\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5679 | Val Loss: 6.9518 | LR: 2.54e-04\n",
      "\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5633 | Val Loss: 6.9663 | LR: 2.51e-04\n",
      "\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5552 | Val Loss: 6.9836 | LR: 2.48e-04\n",
      "\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5473 | Val Loss: 6.9836 | LR: 2.46e-04\n",
      "\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5385 | Val Loss: 6.9615 | LR: 2.43e-04\n",
      "\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5330 | Val Loss: 6.9556 | LR: 2.41e-04\n",
      "\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5273 | Val Loss: 6.9526 | LR: 2.38e-04\n",
      "\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5245 | Val Loss: 6.9567 | LR: 2.36e-04\n",
      "\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5131 | Val Loss: 6.9637 | LR: 2.33e-04\n",
      "\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5043 | Val Loss: 6.9672 | LR: 2.31e-04\n",
      "\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4981 | Val Loss: 6.9619 | LR: 2.29e-04\n",
      "\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4915 | Val Loss: 6.9533 | LR: 2.27e-04\n",
      "\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4864 | Val Loss: 6.9570 | LR: 2.25e-04\n",
      "\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4753 | Val Loss: 6.9729 | LR: 2.23e-04\n",
      "\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4716 | Val Loss: 6.9434 | LR: 2.21e-04\n",
      "\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4622 | Val Loss: 6.9581 | LR: 2.19e-04\n",
      "\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4579 | Val Loss: 6.9483 | LR: 2.17e-04\n",
      "\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4504 | Val Loss: 6.9572 | LR: 2.15e-04\n",
      "\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4405 | Val Loss: 6.9476 | LR: 2.13e-04\n",
      "\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4366 | Val Loss: 6.9539 | LR: 2.12e-04\n",
      "\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4328 | Val Loss: 6.9460 | LR: 2.10e-04\n",
      "\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4259 | Val Loss: 6.9593 | LR: 2.08e-04\n",
      "\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4173 | Val Loss: 6.9647 | LR: 2.07e-04\n",
      "\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4105 | Val Loss: 6.9417 | LR: 2.05e-04\n",
      "\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4024 | Val Loss: 6.9480 | LR: 2.04e-04\n",
      "\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3965 | Val Loss: 6.9450 | LR: 2.02e-04\n",
      "\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3948 | Val Loss: 6.9404 | LR: 2.01e-04\n",
      "\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3826 | Val Loss: 6.9575 | LR: 1.99e-04\n",
      "\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3819 | Val Loss: 6.9450 | LR: 1.98e-04\n",
      "\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3772 | Val Loss: 6.9488 | LR: 1.96e-04\n",
      "\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3684 | Val Loss: 6.9564 | LR: 1.95e-04\n",
      "\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3585 | Val Loss: 6.9625 | LR: 1.94e-04\n",
      "\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3540 | Val Loss: 6.9496 | LR: 1.92e-04\n",
      "\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3526 | Val Loss: 6.9468 | LR: 1.91e-04\n",
      "\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3454 | Val Loss: 6.9504 | LR: 1.90e-04\n",
      "\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3399 | Val Loss: 6.9699 | LR: 1.89e-04\n",
      "\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3342 | Val Loss: 6.9377 | LR: 1.88e-04\n",
      "\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3273 | Val Loss: 6.9412 | LR: 1.86e-04\n",
      "\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3216 | Val Loss: 6.9423 | LR: 1.85e-04\n",
      "\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model with val loss: 6.9595\n",
      "Train Loss: 6.3150 | Val Loss: 6.9595 | LR: 1.84e-04\n",
      "\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model with val loss: 6.9491\n",
      "Train Loss: 6.3125 | Val Loss: 6.9491 | LR: 1.83e-04\n",
      "\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.3083 | Val Loss: 6.9534 | LR: 1.82e-04\n",
      "\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model with val loss: 6.9458\n",
      "Train Loss: 6.3061 | Val Loss: 6.9458 | LR: 1.81e-04\n",
      "\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2986 | Val Loss: 6.9495 | LR: 1.80e-04\n",
      "\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2900 | Val Loss: 6.9557 | LR: 1.79e-04\n",
      "\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model with val loss: 6.9444\n",
      "Train Loss: 6.2882 | Val Loss: 6.9444 | LR: 1.78e-04\n",
      "\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2783 | Val Loss: 6.9520 | LR: 1.77e-04\n",
      "\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2774 | Val Loss: 6.9474 | LR: 1.76e-04\n",
      "\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2678 | Val Loss: 6.9509 | LR: 1.75e-04\n",
      "\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2647 | Val Loss: 6.9553 | LR: 1.74e-04\n",
      "\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2606 | Val Loss: 6.9478 | LR: 1.73e-04\n",
      "\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2587 | Val Loss: 6.9612 | LR: 1.72e-04\n",
      "\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2581 | Val Loss: 6.9634 | LR: 1.71e-04\n",
      "\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2545 | Val Loss: 6.9575 | LR: 1.70e-04\n",
      "\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2509 | Val Loss: 6.9542 | LR: 1.69e-04\n",
      "\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2486 | Val Loss: 6.9626 | LR: 1.68e-04\n",
      "\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2484 | Val Loss: 6.9785 | LR: 1.68e-04\n",
      "\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2495 | Val Loss: 6.9588 | LR: 1.67e-04\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:39:13.869256Z",
     "start_time": "2025-03-17T00:39:13.687992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "dae5f2c464d34d23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:51:29.202587Z",
     "start_time": "2025-03-17T00:51:28.682035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = unescape(str(text))\n",
    "    text = re.sub(r'<\\/?[a-zA-Z_]+\\b[^>]*>', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    text = re.sub(r'[<>]', '', text)\n",
    "\n",
    "    text = text.replace('\"', \"'\")\n",
    "    allowed_punct = r\"!?.,:;'\"\n",
    "    text = re.sub(fr\"[^\\w\\s{allowed_punct}]\", '', text)\n",
    "    text = re.sub(r\"\\s+([!?,.])\\s+\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, is_title=False):\n",
    "    if is_title:\n",
    "        text = re.sub(r'\\b([A-Z]{2,})\\b', r'<KEEP_CAPS>\\1</KEEP_CAPS>', text)\n",
    "        text = titlecase(text.lower())\n",
    "        text = re.sub(r'<KEEP_CAPS>(.*?)</KEEP_CAPS>', lambda m: m.group(1).upper(), text)\n",
    "        return text\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "def spacy_tokenize(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    punct_set = {\".\", \",\", \"!\", \"?\", \":\", \";\", \"'\"}\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tok_text = token.text.strip()\n",
    "        if not tok_text:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        if '_' in tok_text:\n",
    "            continue\n",
    "        if tok_text in punct_set:\n",
    "            tokens.append(tok_text)\n",
    "            continue\n",
    "        if re.match(r'^[a-z0-9]+$', tok_text) and len(tok_text) >= 2:\n",
    "            tokens.append(tok_text)\n",
    "    return tokens\n",
    "\n",
    "def text_to_sequence(tokens, vocab):\n",
    "    return [vocab.get('<start>', 1)] \\\n",
    "        + [vocab.get(tok, vocab.get('<unk>', 1)) for tok in tokens] \\\n",
    "        + [vocab.get('<end>', 1)]\n",
    "\n",
    "def process_inference_desc(text, vocab, nlp):\n",
    "    # 1) clean\n",
    "    text = clean_text(text)\n",
    "    # 2) normalize (desc => is_title=False)\n",
    "    text = normalize_text(text, is_title=False)\n",
    "    # 3) spacy tokenize\n",
    "    tokens = spacy_tokenize(text, nlp)\n",
    "    # 4) map to sequence with <start> / <end>\n",
    "    tokens = [tok if tok in vocab else '<unk>' for tok in tokens]\n",
    "    seq = text_to_sequence(tokens, vocab)\n",
    "    if len(seq) > 20:\n",
    "        seq = seq[:19] + [vocab['<end>']] \n",
    "    return seq"
   ],
   "id": "5cea3d0d4ba4d774",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:51:30.369193Z",
     "start_time": "2025-03-17T00:51:30.362759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def greedy_decode(model, src, src_key_padding_mask, max_len=20):\n",
    "#     \"\"\"Enhanced greedy decoding with correct special token handling\"\"\"\n",
    "#     model.eval()\n",
    "#     device = src.device\n",
    "#     batch_size = src.size(0)\n",
    "#     eos_idx = model.config.eos_idx\n",
    "#     sos_idx = model.config.sos_idx\n",
    "# \n",
    "#     # make sure src_key_padding_mask is boolean\n",
    "#     src_key_padding_mask = src_key_padding_mask.to(torch.bool)\n",
    "# \n",
    "#     # src encoding\n",
    "#     memory = model.encode(src, src_key_padding_mask=src_key_padding_mask)\n",
    "# \n",
    "#     # initialize target with <start>\n",
    "#     tgt = torch.full((batch_size, 1), sos_idx, dtype=torch.long, device=device)\n",
    "# \n",
    "#     # initialize unfinished sequences\n",
    "#     unfinished = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
    "#     final_outputs = [[] for _ in range(batch_size)]\n",
    "# \n",
    "#     for step in range(max_len):\n",
    "#         tgt_mask = model.generate_tgt_mask(tgt.size(1)).to(device)\n",
    "# \n",
    "#         # decode step\n",
    "#         logits = model.decode(\n",
    "#             tgt=tgt,\n",
    "#             memory=memory,\n",
    "#             tgt_mask=tgt_mask,\n",
    "#             memory_key_padding_mask=src_key_padding_mask\n",
    "#         )  # [B, T, V]\n",
    "# \n",
    "#         # get next token\n",
    "#         next_tokens = logits[:, -1, :].argmax(-1)  # [B]\n",
    "# \n",
    "#         # update target sequence\n",
    "#         for i in range(batch_size):\n",
    "#             if unfinished[i]:\n",
    "#                 final_outputs[i].append(next_tokens[i].item())\n",
    "# \n",
    "#         eos_hit = (next_tokens == eos_idx)\n",
    "#         unfinished = unfinished & ~eos_hit\n",
    "# \n",
    "#         # early exit if all sequences have ended\n",
    "#         if not unfinished.any():\n",
    "#             break\n",
    "# \n",
    "#         tgt = torch.cat([\n",
    "#             tgt[unfinished],\n",
    "#             next_tokens[unfinished].unsqueeze(1)\n",
    "#         ], dim=1)\n",
    "# \n",
    "#     # remover token after <end>\n",
    "#     results = []\n",
    "#     for seq in final_outputs:\n",
    "#         if eos_idx in seq:\n",
    "#             eos_pos = seq.index(eos_idx)\n",
    "#             seq = seq[:eos_pos]\n",
    "#         results.append(seq)\n",
    "# \n",
    "#     return results"
   ],
   "id": "46f874e94a8fe79c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:51:30.862402Z",
     "start_time": "2025-03-17T00:51:30.853733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def beam_decode(\n",
    "        model,\n",
    "        src: torch.Tensor,\n",
    "        src_key_padding_mask: torch.Tensor,\n",
    "        max_len: int = 20,\n",
    "        beam_size: int = 5,\n",
    "        length_penalty: float = 0.6\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Beam search decoding with length penalty.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model.\n",
    "        src: Source sequence tensor of shape [1, src_len] (batch_size=1).\n",
    "        src_key_padding_mask: Padding mask tensor for the source [1, src_len].\n",
    "        beam_size: Beam width.\n",
    "        length_penalty: Length penalty coefficient (0.0 ~ 1.0, larger values favor longer sequences).\n",
    "    \n",
    "    Returns:\n",
    "        List[List[int]]: A list of decoded sequences (list length equals beam_size).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = src.device\n",
    "    eos_idx = model.config.eos_idx\n",
    "    sos_idx = model.config.sos_idx\n",
    "\n",
    "    # Encode the source sequence\n",
    "    memory = model.encode(src, src_key_padding_mask=src_key_padding_mask)  # [1, S, D]\n",
    "\n",
    "    # Initialize the beam with a dictionary representing the initial candidate\n",
    "    beams = [\n",
    "        {\n",
    "            'tokens': [sos_idx],\n",
    "            'score': 0.0,\n",
    "            'finished': False\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for step in range(max_len):\n",
    "        candidates = []\n",
    "        for beam in beams:\n",
    "            if beam['finished']:\n",
    "                candidates.append(beam)\n",
    "                continue\n",
    "\n",
    "            # decoder input with the current token sequence [1, current_len]\n",
    "            tgt = torch.tensor([beam['tokens']], dtype=torch.long, device=device)\n",
    "            tgt_mask = model.generate_tgt_mask(tgt.size(1)).to(device)\n",
    "\n",
    "            # Decode to obtain logits\n",
    "            logits = model.decode(\n",
    "                tgt=tgt,\n",
    "                memory=memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_key_padding_mask=src_key_padding_mask\n",
    "            )  # [1, T, V]\n",
    "\n",
    "            # Get logits from the last time step and compute log probabilities\n",
    "            log_probs = torch.log_softmax(logits[0, -1], dim=-1)  # [V]\n",
    "            topk_probs, topk_indices = torch.topk(log_probs, beam_size)  # [beam_size]\n",
    "\n",
    "            # Generate new candidate beams\n",
    "            for i in range(beam_size):\n",
    "                token = topk_indices[i].item()\n",
    "                score = beam['score'] + topk_probs[i].item()\n",
    "                new_beam = {\n",
    "                    'tokens': beam['tokens'] + [token],\n",
    "                    'score': score,\n",
    "                    'finished': (token == eos_idx)\n",
    "                }\n",
    "                candidates.append(new_beam)\n",
    "\n",
    "        # Sort candidates by score with length penalty and keep the top beam_size candidates\n",
    "        candidates.sort(key=lambda x: x['score'] / ((1 + len(x['tokens'])) ** length_penalty), reverse=True)\n",
    "        beams = candidates[:beam_size]\n",
    "\n",
    "        # If all beams have finished decoding, exit early.\n",
    "        if all(beam['finished'] for beam in beams):\n",
    "            break\n",
    "\n",
    "    # Post-processing: remove the <start> token and truncate tokens after <end>\n",
    "    results = []\n",
    "    for beam in beams:\n",
    "        tokens = beam['tokens'][1:]  \n",
    "        if eos_idx in tokens:\n",
    "            eos_pos = tokens.index(eos_idx)\n",
    "            tokens = tokens[:eos_pos]\n",
    "        results.append(tokens)\n",
    "\n",
    "    return results"
   ],
   "id": "d9eb3d6a4604fad7",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:52:40.389873Z",
     "start_time": "2025-03-17T00:52:40.382758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(model, raw_text, vocab, reverse_vocab, nlp, device='cuda',\n",
    "            max_input_len=200, max_target_len=20, beam_size=5):\n",
    "    # process inference description\n",
    "    content_tokens = process_inference_desc(raw_text, vocab, nlp)\n",
    "\n",
    "    # convert to tensor\n",
    "    input_seq = torch.tensor(content_tokens, dtype=torch.long).unsqueeze(0).to(device)  # [1, S]\n",
    "\n",
    "    # padding\n",
    "    if input_seq.size(1) < max_input_len:\n",
    "        pad = torch.full((1, max_input_len - input_seq.size(1)), vocab['<pad>'],\n",
    "                         dtype=torch.long, device=device)\n",
    "        input_seq = torch.cat([input_seq, pad], dim=1)\n",
    "    else:\n",
    "        input_seq = input_seq[:, :max_input_len]\n",
    "        input_seq[0, -1] = vocab['<end>']  # make sure the last token is <end>\n",
    "\n",
    "    # generate src_key_padding_mask\n",
    "    src_pad_mask = (input_seq == vocab['<pad>'])\n",
    "\n",
    "    # generate output sequence\n",
    "    with torch.no_grad():\n",
    "        output_seqs = beam_decode(\n",
    "            model=model,\n",
    "            src=input_seq,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            max_len=max_target_len,\n",
    "            beam_size=beam_size\n",
    "        )\n",
    "\n",
    "    # convert index to tokens\n",
    "    tokens = []\n",
    "    for idx in output_seqs[0]:  # output_seqs[0] \n",
    "        token = reverse_vocab.get(idx, '<unk>')\n",
    "        if token == '<end>':\n",
    "            break\n",
    "        tokens.append(token)\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)  # remove space before punctuation\n",
    "    text = re.sub(r'([.,!?;:])\\s+', r'\\1 ', text)  # space after punctuation\n",
    "    return text.capitalize()"
   ],
   "id": "adb4a001252470b3",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T00:52:43.171252Z",
     "start_time": "2025-03-17T00:52:42.485869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_text_1 = \"tokyo ap a lunar orbiter that japan had planned to launch this year could face further delays, possibly until next year or later, because of a funding shortfall and problems developing the probe's informationgathering capabilities, japan's space agency said wednesday. the japan aerospace exploration agency, or jaxa, released a report to a governmentrun commission explaining expected delays to the launch of the 135 million lunara probe...\"\n",
    "test_text_2 = \"advanced micro devices inc.'s amd inc.'s 90nanometer notebook processors are on their way to customers, according to a research note published by goldman sachs co. thursday.advertisementintroducing ibm eserver p5 systems.powered by ibms most advanced 64bit microprocessor power5tm, p5 systems can run unix and linux simultaneously. learn more about eserver p5 systems.\"\n",
    "\n",
    "for i, test_text in enumerate([test_text_1, test_text_2], start=1):\n",
    "    pred_title = predict(model, test_text, vocab, reverse_vocab, nlp, device=device)\n",
    "\n",
    "    print(f\"===== Input {i} =====\")\n",
    "    print(\"Original text (desc):\", test_text)\n",
    "    print(\"Predicted Title:\", pred_title)"
   ],
   "id": "d5abea685b34d820",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input 1 =====\n",
      "Original text (desc): tokyo ap a lunar orbiter that japan had planned to launch this year could face further delays, possibly until next year or later, because of a funding shortfall and problems developing the probe's informationgathering capabilities, japan's space agency said wednesday. the japan aerospace exploration agency, or jaxa, released a report to a governmentrun commission explaining expected delays to the launch of the 135 million lunara probe...\n",
      "Predicted Title: <unk>, <unk>\n",
      "===== Input 2 =====\n",
      "Original text (desc): advanced micro devices inc.'s amd inc.'s 90nanometer notebook processors are on their way to customers, according to a research note published by goldman sachs co. thursday.advertisementintroducing ibm eserver p5 systems.powered by ibms most advanced 64bit microprocessor power5tm, p5 systems can run unix and linux simultaneously. learn more about eserver p5 systems.\n",
      "Predicted Title: <unk>, <unk>\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31bafda2ae04257"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
