{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T02:29:27.005911Z",
     "start_time": "2025-03-15T02:29:22.677554Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from titlecase import titlecase\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from html import unescape"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:29:27.501595Z",
     "start_time": "2025-03-15T02:29:27.008418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data\n",
    "file_path = r\"C:\\NEU\\7390AdvDS\\assignment3\\Jincheng_Jiang_002333686_code\\AG_news\\train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df[[\"Title\", \"Description\"]].head(1))"
   ],
   "id": "9188cc8e07347b78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
      "\n",
      "                                         Description  \n",
      "0  Reuters - Short-sellers, Wall Street's dwindli...  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:29:28.037593Z",
     "start_time": "2025-03-15T02:29:27.590578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"total numbers: {len(df)}\")\n",
    "print(f\"column name: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\n=== first sample ===\")\n",
    "print(df[[\"Title\", \"Description\"]].head(1))\n",
    "\n",
    "print(\"\\n=== lenth analysis ===\")\n",
    "df['title_len'] = df['Title'].apply(lambda x: len(str(x).split()))\n",
    "df['desc_len'] = df['Description'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"title average lenth: {df['title_len'].mean():.1f} \")\n",
    "print(f\"title max lenth: {df['title_len'].max()} \")\n",
    "print(f\"des average lenth: {df['desc_len'].mean():.1f} \")\n",
    "print(f\"des max lenth: {df['desc_len'].max()} \")"
   ],
   "id": "181453399581af7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total numbers: 120000\n",
      "column name: ['Class Index', 'Title', 'Description']\n",
      "\n",
      "=== first sample ===\n",
      "                                               Title  \\\n",
      "0  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
      "\n",
      "                                         Description  \n",
      "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
      "\n",
      "=== lenth analysis ===\n",
      "title average lenth: 6.8 \n",
      "title max lenth: 19 \n",
      "des average lenth: 31.1 \n",
      "des max lenth: 173 \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# data cleaning",
   "id": "c565762da9ca2256"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:29:31.858864Z",
     "start_time": "2025-03-15T02:29:28.045769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    # remove HTML \n",
    "    text = unescape(str(text))\n",
    "    text = re.sub(r'<\\/?[a-zA-Z_]+\\b[^>]*>', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    text = re.sub(r'[<>]', '', text)\n",
    "\n",
    "    text = text.replace('\"', \"'\")\n",
    "    # Keep basic punctuation (!?.,) and remove other special characters\n",
    "    allowed_punct = r\"!?.,:;'\"\n",
    "    text = re.sub(fr\"[^\\w\\s{allowed_punct}]\", '', text)\n",
    "    # Merge consecutive spaces\n",
    "    text = re.sub(r\"\\s+([!?,.])\\s+\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df['clean_title'] = df['Title'].apply(clean_text)\n",
    "df['clean_desc'] = df['Description'].apply(clean_text)"
   ],
   "id": "670387361d6d1d93",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:29:52.682269Z",
     "start_time": "2025-03-15T02:29:31.917250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(text, is_title=True):\n",
    "    if is_title:\n",
    "        # Step 1: Keep existing all-caps words\n",
    "        text = re.sub(r'\\b([A-Z]{2,})\\b', r'<KEEP_CAPS>\\1</KEEP_CAPS>', text)\n",
    "    \n",
    "        # Step 2: Apply title formatting\n",
    "        text = titlecase(text.lower())\n",
    "    \n",
    "        # Step 3: Restore reserved all-caps words\n",
    "        text = re.sub(r'<KEEP_CAPS>(.*?)</KEEP_CAPS>',\n",
    "                      lambda m: m.group(1).upper(), text)\n",
    "        return text\n",
    "    \n",
    "    else:\n",
    "        # lowercase text\n",
    "        return text.lower()\n",
    "\n",
    "df['norm_title'] = df['clean_title'].apply(lambda x: normalize_text(x, is_title=True))\n",
    "df['norm_desc'] = df['clean_desc'].apply(lambda x: normalize_text(x, is_title=False))"
   ],
   "id": "1c8942800b4f0b78",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:29:54.278287Z",
     "start_time": "2025-03-15T02:29:52.758975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filtering empty values\n",
    "print(f\"nums of beginning: {len(df)}\")\n",
    "df = df.dropna(subset=['norm_title', 'norm_desc'])\n",
    "\n",
    "# Filtering short text\n",
    "df = df[df['norm_title'].str.split().str.len() >= 3]  \n",
    "df = df[df['norm_desc'].str.split().str.len() >= 10]  \n",
    "print(f\"nums after filtering: {len(df)}\")"
   ],
   "id": "cb78112b4bd30ef1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nums of beginning: 120000\n",
      "nums after filtering: 118395\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# tokenize&vocab",
   "id": "1f4cf0c2783c9f73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:41:31.122936Z",
     "start_time": "2025-03-15T02:29:54.380033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "def spacy_tokenize(text):\n",
    "    doc = nlp(text.lower())\n",
    "    punct_set = {\".\", \",\", \"!\", \"?\", \":\", \";\", \"'\"}\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tok_text = token.text.strip()\n",
    "        if not tok_text:\n",
    "            continue\n",
    "\n",
    "        # remove spaCy stop words\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "\n",
    "        # remove underscore\n",
    "        if '_' in tok_text:\n",
    "            continue\n",
    "\n",
    "        # if it is a punctuation, keep it\n",
    "        if tok_text in punct_set:\n",
    "            tokens.append(tok_text)\n",
    "            continue\n",
    "\n",
    "        # length >= 2 and only contains lowercase letters and numbers\n",
    "        if re.match(r'^[a-z0-9]+$', tok_text) and len(tok_text) >= 2:\n",
    "            tokens.append(tok_text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "df['tokenized_title'] = df['norm_title'].apply(spacy_tokenize)\n",
    "df['tokenized_desc'] = df['norm_desc'].apply(spacy_tokenize)"
   ],
   "id": "127d7e5839a030ad",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:43:00.981785Z",
     "start_time": "2025-03-15T02:41:31.143733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_optimized_vocab(df, max_vocab_size=150000, min_freq=3):\n",
    "    # merge Tokens\n",
    "    all_tokens = []\n",
    "    chunksize = 10000  # Batch processing to prevent memory overflow\n",
    "\n",
    "    for i in range(0, len(df), chunksize):\n",
    "        chunk = df.iloc[i:i+chunksize]\n",
    "        all_tokens.extend(chunk['tokenized_title'].sum())\n",
    "        all_tokens.extend(chunk['tokenized_desc'].sum())\n",
    "\n",
    "    # count tokens\n",
    "    counter = Counter(all_tokens)\n",
    "\n",
    "    # Filter low-frequency words\n",
    "    filtered = [\n",
    "        (word, cnt) for word, cnt in counter.items()\n",
    "        if cnt >= min_freq\n",
    "    ]\n",
    "\n",
    "    # Sort by word frequency in descending order\n",
    "    sorted_words = [word for word, _ in sorted(filtered, key=lambda x: (-x[1], x[0]))]\n",
    "\n",
    "    special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "    vocab = special_tokens + sorted_words[:max_vocab_size - len(special_tokens)]\n",
    "\n",
    "    return {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# construct vocab\n",
    "vocab = build_optimized_vocab(df, max_vocab_size=150000, min_freq=3)\n",
    "print(f\"Final Vocab Size: {len(vocab)}\")"
   ],
   "id": "b23efe41b13f55da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocab Size: 37943\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:43:19.172975Z",
     "start_time": "2025-03-15T02:43:17.494129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df[df['tokenized_title'].apply(lambda x: len(x)) >= 3]\n",
    "df = df[df['tokenized_desc'].apply(lambda x: len(x)) >= 10]\n",
    "\n",
    "def text_to_sequence(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in ['<start>'] + tokens + ['<end>']]\n",
    "\n",
    "df['title_seq'] = df['tokenized_title'].apply(lambda x: text_to_sequence(x, vocab))\n",
    "df['desc_seq'] = df['tokenized_desc'].apply(lambda x: text_to_sequence(x, vocab))"
   ],
   "id": "cb07efbe13e25bf5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:43:20.773741Z",
     "start_time": "2025-03-15T02:43:20.103693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_path = r\"C:\\NEU\\7390AdvDS\\assignment3\\Jincheng_Jiang_002333686_code\\AG_news\\processed_data.parquet\"\n",
    "df[['norm_title', 'norm_desc', 'title_seq', 'desc_seq']].to_parquet(processed_path)\n",
    "\n",
    "vocab_path = r\"C:\\NEU\\7390AdvDS\\assignment3\\Jincheng_Jiang_002333686_code\\AG_news\\vocab.csv\"\n",
    "pd.DataFrame(vocab.items(), columns=['word', 'id']).to_csv(vocab_path, index=False)"
   ],
   "id": "1aaed77cdbbab7c3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T02:43:03.101727Z",
     "start_time": "2025-03-15T02:43:03.097517Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c16deab14c8f1f58",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
